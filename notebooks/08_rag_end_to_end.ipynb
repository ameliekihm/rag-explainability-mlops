{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5138d32",
   "metadata": {},
   "source": [
    "## Run in Google Colab notebook: 08_rag_end_to_end.ipynb (Environment: CPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf3d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import os\n",
    "project_root = \"/content/drive/MyDrive/RAG Research\"\n",
    "os.chdir(project_root)\n",
    "print(os.getcwd())\n",
    "\n",
    "!pip install faiss-cpu transformers datasets sentence-transformers accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb5b6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "sys.path.append(\"/content/drive/MyDrive/RAG Research/src\")\n",
    "\n",
    "# 1.Import Retriever and Generator components\n",
    "from src.retrieval.retriever import Retriever\n",
    "from src.generation.generator import Generator\n",
    "\n",
    "# 2. Initialize Retriever\n",
    "retriever = Retriever(\n",
    "    model_name=\"intfloat/e5-large\",\n",
    "    index_path=\"/content/drive/MyDrive/RAG Research/data/processed/hnsw_index.faiss\",\n",
    "    k=20\n",
    ")\n",
    "\n",
    "# 3. Initialize Generator\n",
    "generator = Generator(model_name=\"google/flan-t5-base\")\n",
    "\n",
    "# 4. Run retrieval + generation\n",
    "# Run a sample query to test full pipeline\n",
    "query = \"What is the capital of France?\"\n",
    "indices, distances = retriever.search(query)\n",
    "\n",
    "# 5. Load contexts.json for retrieved data\n",
    "with open(\"/content/drive/MyDrive/RAG Research/data/processed/contexts.json\", \"r\") as f:\n",
    "    contexts = json.load(f)\n",
    "\n",
    "# 6. Combine top-k contexts\n",
    "top_contexts = [contexts[i] for i in indices]\n",
    "combined_context = \" \".join(top_contexts[:3])\n",
    "\n",
    "# 7.Generate final answer\n",
    "answer = generator.generate_answer(query, combined_context)\n",
    "\n",
    "print(\"\\n============================\")\n",
    "print(f\"Q: {query}\")\n",
    "print(f\"A: {answer}\")\n",
    "print(\"============================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dbb810",
   "metadata": {},
   "source": [
    "## End-to-End Answer Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c64f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets faiss-cpu sentence-transformers accelerate -q\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "import torch, json\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1️⃣ Load models\n",
    "print(\"[INFO] Loading models...\")\n",
    "embedder = SentenceTransformer(\"intfloat/e5-large\")\n",
    "generator_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "generator_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\").to(\"cpu\")\n",
    "print(\"[INFO] Models ready\")\n",
    "\n",
    "# 2️⃣ Load dataset (use train split to align question and retrieval domain)\n",
    "dataset = load_dataset(\"squad_v2\", split=\"train[:200]\")\n",
    "contexts = list(set(dataset[\"context\"]))\n",
    "questions = dataset[\"question\"]\n",
    "answers = [a[\"text\"] for a in dataset[\"answers\"]]\n",
    "\n",
    "# 3️⃣ Generate embeddings (for contexts only)\n",
    "context_embeddings = embedder.encode(contexts, convert_to_numpy=True)\n",
    "print(f\"[INFO] Context embeddings shape: {context_embeddings.shape}\")\n",
    "\n",
    "# 4️⃣ Build HNSW index\n",
    "d = context_embeddings.shape[1]\n",
    "index = faiss.IndexHNSWFlat(d, 32)\n",
    "index.hnsw.efConstruction = 200\n",
    "index.add(context_embeddings)\n",
    "index.hnsw.efSearch = 64\n",
    "print(f\"[INFO] HNSW built with {index.ntotal} items\")\n",
    "\n",
    "# 5️⃣ End-to-End evaluation\n",
    "k = 20\n",
    "correct = 0\n",
    "total = len(questions)\n",
    "\n",
    "for i in tqdm(range(total)):\n",
    "    q = questions[i]\n",
    "    q_emb = embedder.encode([q], convert_to_numpy=True)\n",
    "    D, I = index.search(q_emb, k)\n",
    "    top_ctx = \" \".join([contexts[j] for j in I[0][:3]])\n",
    "\n",
    "    # Prompt for generator\n",
    "    prompt = f\"Question: {q}\\nContext: {top_ctx}\\nAnswer:\"\n",
    "    inputs = generator_tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(generator_model.device)\n",
    "    output = generator_model.generate(**inputs, max_new_tokens=80)\n",
    "    pred = generator_tokenizer.decode(output[0], skip_special_tokens=True).lower()\n",
    "\n",
    "    if any(a.lower() in pred for a in answers[i]):\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"\\n✅ Final Accuracy (E5-Large + HNSW + FLAN-T5-Base): {accuracy:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-explain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
