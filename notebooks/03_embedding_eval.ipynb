{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running on Google Colab | Runtime type: GPU (T4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iw7iXZwE9ZD1"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets sentence-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEH5gm9v9xe3"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load SQuAD v2 train\n",
        "squad = load_dataset(\"squad_v2\", split=\"train\")\n",
        "\n",
        "# Use 200 data samples\n",
        "sample = squad.shuffle(seed=42).select(range(200))\n",
        "\n",
        "queries = [q[\"question\"] for q in sample]\n",
        "contexts = [q[\"context\"] for q in sample]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTZB8ggi-dVL"
      },
      "source": [
        "## e5 large - Speed Test & Embedding Shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMazDKMV-miu"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import time\n",
        "\n",
        "model = SentenceTransformer(\"intfloat/e5-large\")\n",
        "\n",
        "start = time.time()\n",
        "embeddings = model.encode(contexts, batch_size=32, show_progress_bar=True)\n",
        "end = time.time()\n",
        "\n",
        "print(\"e5-large shape:\", embeddings.shape)\n",
        "print(\"Time taken:\", round(end-start, 2), \"seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwtQJy6PCKke"
      },
      "source": [
        "## bge-m3 - Speed Test & Embedding Shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93F6Lg5UCL8M"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer(\"BAAI/bge-m3\")\n",
        "\n",
        "start = time.time()\n",
        "embeddings = model.encode(contexts, batch_size=32, show_progress_bar=True)\n",
        "end = time.time()\n",
        "\n",
        "print(\"bge-m3 shape:\", embeddings.shape)\n",
        "print(\"Time taken:\", round(end-start, 2), \"seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYX2bB7SCUMe"
      },
      "source": [
        "## all-mpnet-base-v2 - Speed Test & Embedding Shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbgffVPzCVvq"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "start = time.time()\n",
        "embeddings = model.encode(contexts, batch_size=32, show_progress_bar=True)\n",
        "end = time.time()\n",
        "\n",
        "print(\"mpnet shape:\", embeddings.shape)\n",
        "print(\"Time taken:\", round(end-start, 2), \"seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_5eq1ZvQ5kH"
      },
      "source": [
        "## e5 large - Precision@k, Recall@k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Om1aCI8Qyk2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 1. Load a small sample of SQuAD v2\n",
        "squad = load_dataset(\"squad_v2\", split=\"train[:200]\")  # 200 samples\n",
        "queries = squad[\"question\"]\n",
        "contexts = squad[\"context\"]\n",
        "\n",
        "# 2. Pick model\n",
        "model = SentenceTransformer(\"intfloat/e5-large\")\n",
        "\n",
        "# 3. Embed contexts\n",
        "context_embeddings = model.encode(contexts, batch_size=32, convert_to_tensor=True, show_progress_bar=True)\n",
        "\n",
        "# 4. Function: evaluate retrieval with Precision@k, Recall@k\n",
        "def evaluate(model, queries, contexts, context_embeddings, k=5):\n",
        "    precision_scores = []\n",
        "    recall_scores = []\n",
        "\n",
        "    for i, query in enumerate(queries):\n",
        "        # Query embedding\n",
        "        query_emb = model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "        # Cosine similarity search\n",
        "        hits = util.semantic_search(query_emb, context_embeddings, top_k=k)[0]\n",
        "\n",
        "        # Ground-truth context (the one that contains the answer text)\n",
        "        gold_context = contexts[i]\n",
        "\n",
        "        # Retrieved contexts\n",
        "        retrieved_contexts = [contexts[hit[\"corpus_id\"]] for hit in hits]\n",
        "\n",
        "        # Precision@k: did we retrieve the correct context among top-k?\n",
        "        correct_hits = sum([gold_context in rc for rc in retrieved_contexts])\n",
        "        precision = correct_hits / k\n",
        "        precision_scores.append(precision)\n",
        "\n",
        "        # Recall@k: out of all possible gold contexts, did we cover?\n",
        "        # (here 1 gold context only, so recall is just 1 if found, else 0)\n",
        "        recall = 1.0 if gold_context in retrieved_contexts else 0.0\n",
        "        recall_scores.append(recall)\n",
        "\n",
        "    return np.mean(precision_scores), np.mean(recall_scores)\n",
        "\n",
        "# 5. Run evaluation\n",
        "precision, recall = evaluate(model, queries, contexts, context_embeddings, k=5)\n",
        "print(f\"Precision@5: {precision:.3f}\")\n",
        "print(f\"Recall@5: {recall:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbogCrtVTRrb"
      },
      "source": [
        "## bge-m3 - Precision@k, Recall@k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FN4oQThnTVNT"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "\n",
        "# Load model\n",
        "model = SentenceTransformer(\"BAAI/bge-m3\")\n",
        "\n",
        "# Encode contexts and queries\n",
        "context_embeds = model.encode(contexts, batch_size=32, convert_to_tensor=True, show_progress_bar=True)\n",
        "query_embeds = model.encode(queries, batch_size=32, convert_to_tensor=True, show_progress_bar=True)\n",
        "\n",
        "# Compute cosine similarity\n",
        "scores = util.cos_sim(query_embeds, context_embeds)\n",
        "\n",
        "# Precision@k and Recall@k calculation\n",
        "def precision_recall_at_k(scores, ground_truths, k=5):\n",
        "    precisions, recalls = [], []\n",
        "    for i, row in enumerate(scores):\n",
        "        top_k = row.topk(k).indices.cpu().numpy()\n",
        "        # Assume the i-th context is the ground truth\n",
        "        gt = [i]\n",
        "        correct = len(set(top_k) & set(gt))\n",
        "        precisions.append(correct / k)\n",
        "        recalls.append(correct / len(gt))\n",
        "    return np.mean(precisions), np.mean(recalls)\n",
        "\n",
        "precision, recall = precision_recall_at_k(scores, queries, k=5)\n",
        "print(\"bge-m3 Precision@5:\", round(precision, 3))\n",
        "print(\"bge-m3 Recall@5:\", round(recall, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNz8RnQxTfqr"
      },
      "source": [
        "## all-mpnet-base-v2 - Precision@k, Recall@k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPskHp7wTkdC"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "\n",
        "# Load model\n",
        "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "# Encode contexts and queries\n",
        "context_embeds = model.encode(contexts, batch_size=32, convert_to_tensor=True, show_progress_bar=True)\n",
        "query_embeds = model.encode(queries, batch_size=32, convert_to_tensor=True, show_progress_bar=True)\n",
        "\n",
        "# Compute cosine similarity\n",
        "scores = util.cos_sim(query_embeds, context_embeds)\n",
        "\n",
        "# Precision@k and Recall@k calculation\n",
        "def precision_recall_at_k(scores, ground_truths, k=5):\n",
        "    precisions, recalls = [], []\n",
        "    for i, row in enumerate(scores):\n",
        "        top_k = row.topk(k).indices.cpu().numpy()\n",
        "        # Assume the i-th context is the ground truth\n",
        "        gt = [i]\n",
        "        correct = len(set(top_k) & set(gt))\n",
        "        precisions.append(correct / k)\n",
        "        recalls.append(correct / len(gt))\n",
        "    return np.mean(precisions), np.mean(recalls)\n",
        "\n",
        "precision, recall = precision_recall_at_k(scores, queries, k=5)\n",
        "print(\"mpnet Precision@5:\", round(precision, 3))\n",
        "print(\"mpnet Recall@5:\", round(recall, 3))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
